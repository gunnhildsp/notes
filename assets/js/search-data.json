{
  
    
        "post0": {
            "title": "Data excellence as a first-class citizen of AI",
            "content": "A recent blog post from Vicki Boykis drew me in as it revolves around the topic of implicit knowledge: The ghosts in the data. Implicit knowledge hits us from many differents perspectives nowadays, from automating implicit knowledge through AI, discovering and sharing implicit knowledge in distributed or hybrid workplaces, when enabling beginner coders to develop and maintain applications, and when documenting everything from software applications to model experiments. As all pythonistas know: Explicit is better than implicit. . The blog post pointed me to the paper “Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI. Vicki Boykis refers to this as one of the first formal sources citing explicitly the previously implicit knowledge that data quality matters for ML model performance. Data cascades is defined as compounding events causing negative, downstream effects from data issues, resulting in technical debt over time. The paper is based on a qualitative study of practices among 53 AI practitioners working in high stakes domain, which they define as domains that have safety impacts on living beings, such as climate change and maternal health. . Data workers play a critical role in creating and maintaining AI systems, and their work has downstream impact. . Garbage in :arrow_right: garbage out . Data quality is crucial to ML model performance. Andrew Ng provided a few examples of the importance of data quality in his recent webinar A chat with Andrew on MLOps: From model-centric to data-centric AI. However, this field is quite immature, demonstrated by the fact that the webinar, and deeplearning.ai courses in general, lack introductions to concrete tools and rigorous processes to lean data-centric. . Data cleaning usually involves integrity constraints, type inference, schema matching, outlier detection and more. Several tools have been created to streamline this process, such as Great expectations, Tensorflow data validation and Deequ. An important trait here is to catch the bugs through data validation, not through model performance deterioration: the data quality work will suffer when it is seen as a by-product of model performance monitoring. . Data validation and cleaning focuses on one part of the data lifecycle, but especially in high stakes environment, the birth and afterlife of data needs attion as well, i.e. the data creation, collection process and downstream life of data. We should view data as a dynamic entity with possibility for drifts and skews, requiring follow-up. A further complicating factor is that data quality work is cross-functional in nature: dataset definition and labelling accuracy often depend on subject matter expertice. . Data cascades are hard to spot, and require non-conventional AI practices . According to the paper, data cascades are opaque, meaning that Practitioners were not equipped to identify upstream or downstream issues. They lacked tools and metrics to detect and measure the effects on systems. As a proxy, model performance metrics were used, which test the whole system, not the datasets. . The study also found that “conventional” AI practices are insufficient: Applying “conventional” AI practices in high stakes domains triggers data cascades. Some examples provided are moving fast, hyperparameter tuning to improve model performance instead of data quality work. . Undervaluation of data work manifests in ML metrics, ML prestige and education programs . The paper describes several factors in the ML ecosystem that contribute to data cascades, among the the incentives and currency in AI, and the lack of data education. . Model improvements can be easily tracked and rewarded through model performance metrics. Data improvements are not monitored in the same way. In addition, model development and architectures holds a prestige in the community, whereas data work is neglected. ImageNet, the database of labelled images which has been crucial for computer vision, initially received little recognition as ML work. Fei-Fei Li, the AI researcher who lead the project, struggled to receive funding in the beginning, with rejections commenting that it was shameful that a Princeton professor would research the topic. The undervaluation of data work is not only among practitioners: The study found that there was little buy-in along decision makers and funders for working on data quality improvement. . Lack of focus on data in education programs left practitioners at a loss when faced with the realities of data in the wild: “In real life we never see clean data,” one of the subjects said. In spite of this, training and education provides little guidance on how to collect, clean and document datasets, let alone handle live data. The study included both US practitioners, mostly from AI specialisations in graduate programs, and Indian, West African and East African practitioners, who were mostly self-taught after completing CS degrees. Data engineering was under-emphasized in both routes. . Categories of data cascades . The paper provides detailed descriptions of the four most prevalent categories of data cascades, including many disheartening citations from the practitioners. . Physical-world brittleness was the most common, and is caused by changes in the world we are modelling, such as hardware drifts, changes in environment, human behaviour or legislation. Mitigations to physical-world brittleness cascades included: Monitoring data sources (often at an example level) | Introducing noise in the training dataset to overcome the disparage between pristine training data and messy live data | Investing in data literacy for system operators and field partners as also introduced in a few cases | . | Inadequate domain expertise: When AI practitioners were tasked with defining ground truth, identifying the necessary feature set and interpreting data, data cascades were triggered. These errors were very costly, often only discovered after model deployment. Consequences ranged from requiring additional data collection, addition of new data sources to entire data collection pipelines needed to be reworked. The study gives two important subclasses of inadequate domain expertise: Subjectivity in ground truth, leading to ambiguous labels, and defining and collecting representative data. Mitigations included: End-to-end domain expertise involvement, not only in early stages or trouble shooting situations, as many projects do, but deep involvement. | Defining representative datasets particular to the domain and problem definition at hand is also crucial | . | Concflicting reward systems: Conventional AI practices viewing data collection as a non-technical task to be outsourced to field partners, caused problems in high stakes domains. Common problems were field partners missing incentives for data quality and having conflicting tasks. In addition, field partners often had poor data literacy, and didn’t understand the impact of the data collection process. Mitigating measures included Data literacy training, however, this was very rarely provided. When training was given, data quality was reported to improve. One practitioner reported providing real time feedback on data quality indicators to field partners, as a training. | Partnerships with field partners, lead by top level management, could have improved the incentive and task conflicts, but most practitioners reported that top level management did not prioritize this. | . | Poor cross-organisational documentation: Conventional AI practice of neglecting data documentation is especially damaging in domains where the volume of data is low. Documentation practices have been suggested, but are not widely implemented. These cascades were often discovered through chance through manual review. Many practitioners also complained of lack of standards when entering data, and issues with vendor documentation for data collecting equiment. Mitigating these issues involved clear procedures and meticulous attention: Data collection plans | Data strategy handbooks | Design documents | File conventions | . | . The problem can be avoided . Step-wise and early interventions in the development process enables practicioners to avoid data cascades, according to the investigation. However, this was rare, due to undervaluation of data work and partner dependencies. Katie Bauer recently connected early intervention in reminds of Katie Bauer’s thread on how the concept shifting left, used about information security work in the book Accelerate, applies to data science and data practices: . twitter: https://twitter.com/imightbemary/status/1371857703077736454?s=20 . In addition to the mitigating actions individuals can take, we need to shift our reactive processes, viewing data as “grunt work”, to a proactive focus on data excellence. The paper suggests three improvement areas for education, conferences and organisations to begin to move the needle on the prestige of data work. In addition, the practices of well performing practitioners were . What should you change tomorrow? Incorporate best software practices adapted to data . Data cascades surface the need for several feedback channels at different stages in the AI lifecycle. The teams with the fewest data cascades showed some common practices: . step-wise feedback loops throughout the development iterations | frequenct model runs | close collaboration with domain experts and field partners | maintained clear data documentation | regular monitoring of incoming data | . These practices mirror software development practices adapted to data. This reduces uncertainty and data cascades building up. . Research is required to go from goodness-of-fit to goodness-of-data . Today, the community is overly reliant on goodness-of-fit metrics. This leads to creating models that fit well to training data, but ignoring the issue of whether the data fits the real world phenomenon we are modelling. In addition, the metrics are only available at a later stage in development, but course correction for data sources should start as early as possible. This could prevent data cascades, and ensure data can be used for several applications. Measuring goodness-of-data will also allow improvements. That being said, metrics are always problematic, and as of now, research on goodness-of-data metrics is immature. . Organisations and conferences should incentivise data excellence . Scientific and purposeful processes outlined in human centered computing contrast the reality of the AI practitioners, who tended to view data as operations, far from the glamour of model work. Conferences should be part of the necessary culture change, accepting and highlighting data exellence. Organisations should reward data collection, pipeline maintenance, data documentation etc. in promotions and peer reviews. Data work is currently lopsided, with domain experts and field partners doing one-off inconvenient jobs. Data exellence emphasizes partnerships and deep involvement of domain experts, sharing credit for work should only be natural. . AI education should provide real world data literacy training . The practical data skill gap induced by pristine practice dataset in education is substancial. Training on data collection, curation and inter-disciplinary collaboration can help prepare future practitioners. There is a large body of work from human centered computing and data ethics to draw on. . Additions to the reading list . The paper cites a lot of interesting work. I noted in particular the following papers: . Practitioners have shown to collaborate much less around datasets, relative to collaboration around code: How do data science workers collaborate? Roles, workflows and tools by Amy X Zhang, Michael Muller, and Dakuo Wang. | Melanie Feinberg describes data as a design material and our role as designers of data, not it’s appropriators: A design perspective on data by Melanie Feinberg. | Muller et al. extend and outline five approaches of data scientists to perform analyses: How data science work-ers work with data: Discovery, capture, curation, design, creation, by Muller, Lange, Wang, Piorkowski, Tsay, Liao, Dugan and Erickson. | .",
            "url": "https://gunnhildsp.github.io/notes/data-quality/2021/03/27/data-cascades.html",
            "relUrl": "/data-quality/2021/03/27/data-cascades.html",
            "date": " • Mar 27, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Graphical inference",
            "content": "Exploratory data analysis is often geared towards finding new patterns simply from looking around, whereas statistical inference focuses on rejecting patterns that are spurious. In a 2010 paper titled Graphical inference for infovis, Hadley Wickham, Dianne Cook, Heike Hofmann and Andreas Buja suggest a visualization technique allowing us to do visual hypothesis testing. The paper is a good read, written in clear language, and uses their suggested visualization framework to create puzzles for the reader. . How do we convict data sets? . The paper goes all out on the analogy of convicting data sets: In the statistical justice system, a data set plays the role of the accused, and the accusation is the hypothesis we wish to test, for example that a medical treatment is effective. The test statistic, our p value, is the evidence that we compare to a standard to convict or aquit the dataset. In the statistical justice system, we compare the accused to the population of innocents which we call the null distribution. The innocents are generated from the null hypothesis and test statistic. The guilt of the accused is determined by comparing the accused with the fraction of the population of innocents which look more guilty than the accused. . In visual testing, the test statistic and the evaluation of similarity with innocents differs: Instead of a test statistic we will use a plot of the data, and instead of a mathematical measure of difference, judgement will be left to a human judge (or even a jury). In visual testing, we use a null dataset, a sample from the null distribution, and a null plot, a plot of a null distribution. This shows us what an innocent data set might look like. We will present our human judge or jury with a plot of the accused dataset, among several null plots, and see if the judge can spot the accused. If they can&#39;t, we fail to reject the null hypothesis, but do not instantly take on the position that the accused is innocent. If they can, there is evidence that the accused is not innocent. . The article does not suggest that visual testing should replace statistical hypothesis testing, but that in exploratory data analysis, where testing is perhaps not the standard, visual testing provides a framework for skepticism. The consequence of convicting a guilty data set, i.e. a false positive, is usually low in exploratory analysis, so we can allow ourselves a few more errors. Visual testing can also meet the needs of complex exploratory analysis for which there is no corresponding numerical test. . Using Rorschach and line-ups for graphical inference . The paper suggests two different tools that are useful when performing visual testing: The Rorschach and the line-up. . The Rorschach . The Rorschach provides a plot like the one below, and encourages the reader to take a moment to study the plots and consider what they tell us. This visualization is named after the Rorschach test as it is posed as an open question to subject: What do you see? . The twist is that the data is all generated from the same distribution - the uniform distribution between 0 and 1, so any patterns spotted in the plots above are spurious relationships. The Rorschach should be used to calibrate to natural variation. . The paper suggests to set a test up with an administrator providing plots and asking questions, and an analyst answering. One could also use software instead of an administrator, and the paper suggests using this a calibrating exercise. If we include plots of real data inbetween null distribution plots, we can hopefully avoid the analyst tiring. . The line-up . In the line-up, we measure similarity between the accused data set and the innocents visually instead of numerically. We line up several null plots along with the accused data set, for a human judge or jury to evaluate. If the accused data set can be distinguished from null plots, there is some evidence that the data set is guilty. If we cannot identify the accused dataset, we do not conclude that the data set is innocent, rather we say that we fail to reject the null hypothesis. . A line-up might look something like the plot below, where the true data plot compares the yearly temperature of the reference period of the Meteorological Institute, 1961 - 1990, to the yearly temperature of the following 30 years. The data was fetched from Norsk Klimaservicesenter. The true data is mixed in with plots of data resampled from the period 1961 - 2019. The null hypothesis is that the temperature distribution in the reference period and the following thirty years are equal. . Resampling data seems to be the simplest strategy for creating null data when our hypothesis is that two groups are different. In this case the groups are the reference period, 1961 - 1990, and the following years, 1991 - 2019. If we transform our data set to tidy data, where each variable is a column and each observation is a row, we can resample the data by randomly permuting one of the columns. Any dependence between the groups will be broken by the permutation. (The Tidy data paper is written by one of the authors of the Graphical inference for infovis authors, Hadley Wickham, of R and tidyverse fame.) . Other use cases for resampling one column in the data to create null datasets: . If we want to investigate spatial correlation on a map, the null hypothesis can be that location and value are independent. We can permute the value column to generate null datasets. | Scatter plots can be used for investigating correlation between $x$ and $y$. If our null hypothesis is that $x$ and $y$ are independent, we can permute either the $x$ or $y$ column to create null datasets. | If we want to investigate correlation between $x$ and $y$ between a number of different groups, we might visualize the data in a scatter plot of $x$ and $y$, with different colors for different groups. The null hypothesis is that groups and position are independent, so to generate a null dataset, we can permute the group id column. | . Another example of a plot of the same temperature data can explore whether or not year and temperature are independent: . Here, the temperature column has been resampled for the null plots, and there is no splitting into groups. The hypothesis that year and temperature is independent is perhaps too strict to provide valuable input. However, it is interesting to see the spurious patterns we falsely identify in the null plots: Plot 4 has an upward trend at the beginning of the period, whereas plot 2 has an upward trend at the end of the period. . Often we will find that the assumption that two variables are independent is too strong: in some settings it is obvious that the two variables are related, following a specific example. If we want to test more complex hypothesis, for example that our data follows a certain model, we can simulate data from the model for our null plots. A typical example would be that the residuals in a regression model are normally distributed. We could then generate null datasets from the normal distribution to mix with our real data. . What is the probability of correctly convicting a guilty dataset with our new tools? . First and foremost, it is important that the analyser of the data does not see the data before making guesses about the true distribution, for inferential validity. We can rely on an independent analyst or write software to ensure the analyst does not need to see the data. . In a practical setting, the paper recommends to use 19 null plots along with one plot from our accused data set. Then the probability of picking the accused data set, if innocent, is 1/20 = 0.05, a traditional boundary for statistical significance. A larger number of plots give a smaller p value, but also leads to viewer fatigue. . Instead of increasing the number of plots, we can increase the number of people trying to find the real data set among the plots, gathering a jury of analyst instead of a single judge. Imagine we have $K$ jurors. If $k$ spot the real data, our p value is $P(X leq k)$, where X follows a binomial distribution $B(n, p)$ with parameters $n=K$ and $p=0.05$ if we use 20 plots. If all jurors spot the data set, our p value will be $0.05^K$. . In reality, an effective visualization will help the analyst spot the true data, and a poor visualization can make it more difficult. In any case, the techniques in this paper describes a few ways of safeguarding against spurious pattern finding. Calibration to variability in data is also an interesting practice for audience looking at unfamiliar data. In my opinion, it is also well written and teaches a thing or two about engaging the audience. . Next on the reading list . Next up on my reading list on this topic is Jessica Hullman and Andrew Gelman&#39;s 2020 paper on Interactive analysis needs theories of inference, which discusses Bayesian model checking. They assume people perform some model checking when examining graphs, against a pseudo-statistical mental model of the data, and explore some implications for visualizations, among other things. The paper describes the line-up as a special case of Bayesian model checking, where graphs are examined as hypothesis testing, comparing data to a null hypothesis. .",
            "url": "https://gunnhildsp.github.io/notes/visualization/statistics/2021/03/01/graphical-infovis.html",
            "relUrl": "/visualization/statistics/2021/03/01/graphical-infovis.html",
            "date": " • Mar 1, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Testing with pytest",
            "content": "You should test your code . There are many reasons why you should test your code: . Writing a test helps you define what your code should do and helps you enforce single responsibility of functions. | Getting into a habit of writing tests for corner cases helps you develop more robust code. | If your tests have good names and reasonable test cases it serves as documentation of your code for your future self or other collaborators. | It is a good way of documenting assumptions you make. It is often useful to write a test that fails in the event of an assumption being broken, such as functionality you have not implemented yet. | Many IDEs have good built-in support for running tests, and debugging a test is often my main way of either debugging through my own code to find errors, or stepping through code that is unfamiliar to me to se how the code is supposed to work and fail. | Good test coverage is essential when refactoring code. | . Put your tests where pytest can find them . pytest finds your tests automatically according to (what I have just learned is) standard test discovery by: . recursively looking through directories | search for files named test_*.py or *_test.py | in those files: search for functions prefixed with test outside classes | in those files: search for functions or methods inside classes prefixed with Test | . An example of a directory structure could look like this: . my_code/ app.py utils.py tests/ test_app.py test_utils.py . It is good practice to organize your tests separately from the rest of your code, for example in a folder named tests as above. There are many reasons, for example default module discovery may ignore your tests, your tests may require additional packages to run, and if you are writing a library or application, the tests should not need to be included in your library or application. . Run your tests . I normally use PyCharm and find the built-in functionality for running and debugging tests from the interface quite nice, but it is always cool to learn more CLI tricks. . Run all tests . Running all tests found from the current directory is quite simple: . !python -m pytest . ============================= test session starts ============================== platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1 rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini collected 6 items test_examples.py .F [ 33%] test_fixturefunctions.py . [ 50%] test_mark_examples.py ... [100%] =================================== FAILURES =================================== _____________________________ test_failing_example _____________________________ def test_failing_example(): print(&#34;Hello&#34;) &gt; assert False E assert False test_examples.py:9: AssertionError -- Captured stdout call -- Hello =========================== short test summary info ============================ FAILED test_examples.py::test_failing_example - assert False ========================= 1 failed, 5 passed in 0.83s ========================== . Note: We can run the tests by running either python -m pytest or just pytest. Running through python will add the current directory to sys.path which is often desirable, therefore I&#39;ll stick with that. . Here, pytest discovered three file with tests, test_examples.py, containing two tests, one which passes and one which fails, test_fixturefunctions.py, containing one passing test and test_mark_examples.py, containing three passing tests. The tests in test_examples.py look like this: . # contents of test_examples.py def test_example(): print(&quot;Hi&quot;) assert True def test_failing_example(): print(&quot;Hello&quot;) assert False . Customize test output . The default mode is that output from a test is not shown unless the test fails. We can use the capture option to print output anyway, or -s for short: . !python -m pytest test_examples.py::test_example --capture=no . ============================= test session starts ============================== platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1 rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini collected 1 item test_examples.py Hi . ============================== 1 passed in 0.01s =============================== . The traceback formatting for failing tests is set by the option tb. There are many options, such as --tb=line to limit output from failing tests to one line: . !python -m pytest test_examples.py --tb=line . ============================= test session starts ============================== platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1 rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini collected 2 items test_examples.py .F [100%] =================================== FAILURES =================================== /Users/Gunnhild/code/notes/_notebooks/test_examples.py:9: assert False =========================== short test summary info ============================ FAILED test_examples.py::test_failing_example - assert False ========================= 1 failed, 1 passed in 0.06s ========================== . Specify modules, files or single tests . Above we used a trick: We can run tests found in a single file and a single test from the command line as well, using the syntax . pytest test_module/test_file_name.py::test_function_name . !python -m pytest test_examples.py::test_example . ============================= test session starts ============================== platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1 rootdir: /Users/Gunnhild/code/notes/_notebooks collected 1 item test_examples.py . [100%] ============================== 1 passed in 0.01s =============================== . Group tests using marks . We can use marks to run groups of tests easily with the -m option . python -m pytest -m mark_name . pytest has a range of built-in marks, such as the slow mark. This can be used to group tests so that you can run the quick tests and check for failures there first, before running the slow tests. We register our marks in pytest.ini to let pytest know we are marking on purpose, otherwise pytest will raise a Warning. For example, we could mark tests for different purposes: . # contents of test_examples.py import pytest @pytest.mark.this def test_example(): print(&quot;Hello&quot;) assert True @pytest.mark.this @pytest.mark.that def test_several_marks(): print(&quot;Nothing&quot;) assert True def test_unmarked(): print(&quot;Hello&quot;) assert 1 . Our pytest.ini should then look like . # content of pytest.ini [pytest] markers = this: example of marker. that: another example of marker. . and then we can run groups accordingly: . !python -m pytest -m this . ============================= test session starts ============================== platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1 rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini collected 6 items / 4 deselected / 2 selected test_mark_examples.py .. [100%] ======================= 2 passed, 4 deselected in 0.75s ======================== . !python -m pytest -m &quot;this and not that&quot; . ============================= test session starts ============================== platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1 rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini collected 6 items / 5 deselected / 1 selected test_mark_examples.py . [100%] ======================= 1 passed, 5 deselected in 0.59s ======================== . Use fixtures to initialize your test . In software testing, a fixture can be used to ensuring that tests are repeatable: the same code with the same inputs in the same environment will reproduce the same results. We can use fixtures for . setting up mocks of external services such as APIs, so your tests won&#39;t depend on the reliability of external applications, and your can test all response cases you need to | setting up and sharing test data between tests | setting up the environment that the test will run in | . In this example, we have a function that saves an input dataframe to a specified path as a csv file. . from pathlib import Path import pandas as pd def save(df: pd.DataFrame, save_path: Path): if not df.empty: df.to_csv(save_path) else: print(&quot;Nothing to save. &quot;) . To test this function, we might want to save a dataframe, and check that we get the same result back when we read the csv file. For this test we use the built-in fixture tmp_path. The tmp_path fixture creates a path unique to each test run, that doesn&#39;t clutter the repository or any other shared folders we might care about. This ensures that if the tests are run in a different environment, such as on another developer&#39;s computer or in a continuous integration pipeline, the folders will exist when needed and be deleted eventually. We use any fixture in a test by using the fixture name as an input argument to the test function: . import numpy as np import pandas as pd from pandas.testing import assert_frame_equal def test_save(tmp_path): # Given save_path = tmp_path / &quot;df.csv&quot; df_expected = pd.DataFrame(columns=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], data=[3, 2, 1]*np.ones([5,3])) save(df_expected, save_path) # When df_actual = pd.read_csv(save_path, index_col=False) # Then assert_frame_equal(df_expected, df_actual) . !python -m pytest test_save_example.py::test_save . ============================= test session starts ============================== platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1 rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini collected 1 item test_save_example.py . [100%] ============================== 1 passed in 0.96s =============================== . Create test data in fixtures . Right now we create the test data in the test. An alternative is to create the dataframe in a fixture. The advantage is that there is less code to read in the test, and the fixture can be reused by different tests, if we have several functions acting on the data we can avoid duplication of code. We create fixtures by using the decorator @pytest.fixture: . import numpy as np import pandas as pd from pandas.testing import assert_frame_equal from pathlib import Path import pytest @pytest.fixture() def test_dataframe(): df = pd.DataFrame(columns=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], data=[3, 2, 1]*np.ones([5,3])) return df def test_save_fixturized(tmp_path, test_dataframe): # Given save_path = tmp_path / &quot;df.csv&quot; save(test_dataframe, save_path) # When df_actual = pd.read_csv(save_path, index_col=False) # Then assert_frame_equal(test_dataframe, df_actual) . !python -m pytest test_save_example.py::test_save_fixturized . ============================= test session starts ============================== platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1 rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini collected 1 item test_save_example.py . [100%] ============================== 1 passed in 0.84s =============================== . Note that when we call the fixture function, we automatically get the return value, instead of the function itself, i.e. we do not need to use assign the return value of the function to a variable holding the dataframe: df_expected = test_dataframe() . Use monkeypatch to set environment variables . We often use environment variables to configure our functionality, such as where they should output their results, login credentials for databases and services. Keeping these configs in environment variables is recommended in order to run the same code with different configurations in different environments: locally when developing, in a test environment and in a production environment. To test these functions, we can use monkeypatching. Let&#39;s say we read environment variables in our function: . import os def read_config(): password = os.environ[&quot;DB_PASSWORD&quot;] user = os.environ[&quot;DB_USER&quot;] return { &quot;password&quot;: password, &quot;user&quot;: user, } . We can then use the monkeypatch fixture in our test, to set environment variables to toy values for the test execution: . def test_read_config(monkeypatch): monkeypatch.setenv(&quot;DB_PASSWORD&quot;, &quot;password123&quot;) monkeypatch.setenv(&quot;DB_USER&quot;, &quot;username&quot;) conf = read_config() assert set(conf.keys()) == {&quot;password&quot;, &quot;user&quot;} assert conf[&quot;password&quot;] == &quot;password123&quot; assert conf[&quot;user&quot;] == &quot;username&quot; . !python -m pytest test_monkeypatching.py::test_read_config . ============================= test session starts ============================== platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1 rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini collected 1 item test_monkeypatching.py . [100%] ============================== 1 passed in 0.02s =============================== . We could extract the mocking into fixtures to share the setup between tests: . import pytest @pytest.fixture() def monkeypatch_config(monkeypatch): monkeypatch.setenv(&quot;DB_PASSWORD&quot;, &quot;password123&quot;) monkeypatch.setenv(&quot;DB_USER&quot;, &quot;username&quot;) def test_read_config_using_fixture(monkeypatch_config): conf = read_config() assert set(conf.keys()) == {&quot;password&quot;, &quot;user&quot;} assert conf[&quot;password&quot;] == &quot;password123&quot; assert conf[&quot;user&quot;] == &quot;username&quot; . !python -m pytest test_monkeypatching.py::test_read_config_using_fixture . ============================= test session starts ============================== platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1 rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini collected 1 item test_monkeypatching.py . [100%] ============================== 1 passed in 0.02s =============================== . Use mocks to test external dependencies . When we have external dependencies, such as an API or databases, we want our tests to be independent of the status of our dependencies. For instance, we want to test that our code can handle both when the API is up and running normally, and when the API is down. However, we can&#39;t control whether the API is up or down when we run our tests, so we use mocks to imitate the responses from our dependencies. . Mocking is a field big enough for it&#39;s own post at some point, but what I keep coming back to is a RealPython article on Understanding the Python Mock Object Library. . Use parametrization to cover multiple cases . There are at least two ways of rerunning tests for different test cases in order to ensure all execution paths are tested, and both involve parametrizing: . Parametrizing fixtures | Parametrizing tests | . When we parametrize, pytest will run the tests for all different cases we specify automatically. . In my experience, we should parametrize tests to ensure that we cover all the different cases that arise from having different input data to the function under test, i.e. the function specific stuff, whereas we should parametrize fixtures when we want to test different objects. If the fixtures are mocking external dependencies or our own complex objects, it may be a good idea to parameterize fixtures to ensure we cover different setups. . A code smell indicating that we should parametrize a fixture, is duplicated code for creating different tests for different functions, or setting up different test cases in the same test, across multiple tests. A nice side effect of parametrizing your fixtures, is that all new tests that use the same fixture will automatically be run for the different cases. . Parametrizing fixtures to cover multiple test cases . Let&#39;s go back to the save test example of saving a dataframe. . Note: In this test, I have parametrized an input parameter to the function, but above I argued that input arguments is better suited for test parametrization than fixture parametrization. A better example would perhaps be if the data in the test was an attribute of a class, and we wished to create a mock of the class to test. It may also be suitable to extract input parameters to fixtures when creation is complex. In any case, the example serves to show some of the functionality of fixtures that we can use. . Where we left off, our test only covered one execution path: the first branch of the if statement, i.e. if the input dataframe is non-empty. If we want to test the other branch, we can parametrize the fixture to return different dataframes. When a test relies on a parametrized fixture, it will be rerun for all parametrizations of the fixture. . import numpy as np import pandas as pd from pandas.testing import assert_frame_equal import pytest @pytest.fixture(params=[True, False], ids=[&quot;non-empty&quot;, &quot;empty&quot;]) def dataframes(request): if request.param: return pd.DataFrame(columns=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], data=[3, 2, 1]*np.ones([5,3])) else: return pd.DataFrame() def test_save_parametrized_fixture(tmp_path, dataframes): # Given save_path = tmp_path / &quot;df.csv&quot; save(dataframes, save_path) if dataframes.empty: # When files_in_dir = [x for x in tmp_path.iterdir() if x.is_file()] # Then assert not files_in_dir else: # When df_actual = pd.read_csv(save_path, index_col=False) # Then assert_frame_equal(dataframes, df_actual) . !python -m pytest test_save_example.py::test_save_parametrized_fixture . ============================= test session starts ============================== platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1 rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini collected 2 items test_save_example.py .. [100%] ============================== 2 passed in 0.60s =============================== . This executes the test twice automatically. We use the params keyword to parametrize our fixture, and the ids keyword to provide human readable names for our different parametrizations. We use the request fixture in our fixture to access the parameters we send in on the request&#39;s attribute param. . params takes a list as inputs, so if we need several arguments to our fixture function, we can use for example a list of tuples or a list of dicts: . @pytest.fixture(params=[(True, 5), (False, )], ids=[&quot;non-empty&quot;, &quot;empty&quot;]) def df_fixture_with_tuples(request): if request.param[0]: n = request.param[1] return pd.DataFrame(columns=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], data=[3, 2, 1]*np.ones([n,3])) else: return pd.DataFrame() @pytest.fixture( params=[ {&quot;non_empty&quot;: True, &quot;length&quot;: 5}, {&quot;non_empty&quot;: False, &quot;length&quot;: None} ], ids=[&quot;non-empty&quot;, &quot;empty&quot;] ) def df_fixture_with_dict(request): if request.param[&quot;non_empty&quot;]: n = request.param[&quot;length&quot;] return pd.DataFrame(columns=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], data=[3, 2, 1]*np.ones([n,3])) else: return pd.DataFrame() . Lets run a failing test, to see our id in action, with this toy test function . def test_demo_fail_output(dataframes): if dataframes.empty: assert False else: assert True . !python -m pytest --tb=line test_save_example.py::test_demo_fail_output . ============================= test session starts ============================== platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1 rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini collected 2 items test_save_example.py .F [100%] =================================== FAILURES =================================== /Users/Gunnhild/code/notes/_notebooks/test_save_example.py:74: assert False =========================== short test summary info ============================ FAILED test_save_example.py::test_demo_fail_output[empty] - assert False ========================= 1 failed, 1 passed in 0.76s ========================== . The id of the failing test, empty, is printed in the list of failed tests. If you use PyCharm, you will find that it prints a pretty summary of the ids of parametrized tests, both parametrized through fixtures and the test itself, by building up a tree of the tests that are run, organised by module, script, and function, and I&#39;m sure many other IDEs have similar functionality. . Parametrize tests to cover multiple test cases . To cover the different execution paths, we can also parametrize the test itself, which looks a little different. Let&#39;s return to our save example, but add to the functionality. Let&#39;s say we want to pass an argument for the number of rows to save, and add a validator to check that the number of rows is a valid argument: . import pandas as pd def save(df: pd.DataFrame, save_path: Path, num_rows: Optional[int] = None): if not df.empty: if num_rows: num_rows = validate_num_rows(num_rows) df = df[0: num_rows] df.to_csv(save_path, index=False) else: print(&quot;Nothing to save. &quot;) def validate_num_rows(num_rows)-&gt; int: if not int(num_rows) == num_rows: raise ValueError(f&quot;num_rows must be int, got {num_rows}&quot;) if num_rows &lt; 1: raise ValueError(f&quot;num_rows must be &gt;= 1, got {num_rows}&quot;) return int(num_rows) . We parametrize our test to cover both the case when a num_rows argument is not supplied, and when it is supplied: . import pandas as pd import pytest @pytest.mark.parametrize(argnames=&quot;number_of_rows&quot;, argvalues=[None, 3]) def test_save_fixturized(tmp_path, test_dataframe, number_of_rows): save_path = tmp_path / &quot;df.csv&quot; save(test_dataframe, save_path, num_rows=number_of_rows) df_actual = pd.read_csv(save_path) if number_of_rows: df_expected = test_dataframe[0:number_of_rows] else: df_expected = test_dataframe assert_frame_equal(df_expected, df_actual) . Parametrize is a mark, where the first argument, argnames, is a string with the argument names separated by commas, the second, argvalues is a list with the argument values for the different test cases. If we have several arguments, argvalues must be a list of tuples, and the number of tuples must match the number of argnames for each element of the list. We use the parameterized values in the test by setting them as input arguments to the test. These names must match argnames. . !python -m pytest test_parametrize.py::test_save_fixturized . ============================= test session starts ============================== platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1 rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini collected 2 items test_parametrize.py .. [100%] ============================== 2 passed in 0.79s =============================== . Testing that exceptions are raised . To make assertions about expected exceptions, we use pytest.raises. We will use the function validate_num_rows as an example, as it raises errors in some cases, and not in others. This is also a good opportunity to document some assumptions for our future self about what this test does. Since there are many different cases, we will parametrize the test function to cover all branches of the code and demonstrate functionality. . import pytest @pytest.mark.parametrize( &quot;input_num_rows, expected_output, expected_error&quot;, [ (3, 3, None), (2.0, 2, None), (&quot;text&quot;, None, ValueError), (&quot;3.4&quot;, None, ValueError), (-1, None, ValueError), ], ids=[ &quot;integer&quot;, &quot;float_that_can_be_converted_to_integer&quot;, &quot;string_fails&quot;, &quot;float_fails&quot;, &quot;negative_number_fails&quot; ] ) def test_validate_num_rows(input_num_rows, expected_output, expected_error): if expected_error: with pytest.raises(expected_error): validate_num_rows(input_num_rows) else: actual_output = validate_num_rows(input_num_rows) assert expected_output == actual_output . Now we have an example of having multiple argument names and argument values with tuples, as mentioned above. . To test for failure and success, we use the argument expected_error which we set to None for the test cases that should fail and to the error we expect when a test should pass. Then we use pytest.raises to call a function and validate that the expected error was thrown if expected_error is not None. . !python -m pytest test_parametrize.py::test_validate_num_rows . ============================= test session starts ============================== platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1 rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini collected 5 items test_parametrize.py ..... [100%] ============================== 5 passed in 0.56s =============================== . Another option for conditional raising of exeptions is shown in the documentation, and uses a contextmanager that yields for non-failing cases. It seems a little complicated to me, but if you&#39;re used to this construction you can save some lines of code in your tests. . Further reading . Mocking is an obvious next step when writing tests, my favorite source is the above mentioned RealPython article on Understanding the Python Mock Object Library. . Code coverage is a concept that goes hand in hand with testing and is a good starting point for what to test. pytest-cov is an easy coverage plugin for pytest. . The ecosystem of plugins to pytest is huge, and there are many I would like to try, for examplepytest-mock for mocking and pytest-vcr for HTTP requests. This tutorial covers both the pytest-vcr library, but also basic concepts in testing and code quality, as well as the author&#39;s strategy on how to read up on testing in Python. .",
            "url": "https://gunnhildsp.github.io/notes/python/pytest/testing/2020/04/12/pytest.html",
            "relUrl": "/python/pytest/testing/2020/04/12/pytest.html",
            "date": " • Apr 12, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Challenges with simple metrics in machine learning",
            "content": "The challenge with simple evaluation metrics in machine learning . Picking a metric for your problem implies defining success. This makes it important to know different metrics, their shortcomings and possible mediating actions when metrics are not sufficient. There has been many interesting pieces on this topic lately. My sources for this post are: . Data scientists: beware of simple metrics: Episode of the podcast Linear Digressions that provides a gentle introduction to the topic, and was my source for the articles below. | [1]: Reliance on Metrics is a Fundamental Challenge for AI: Reviews of different case studies showing how emphasis on metrics can lead to manipulation, gaming of scores and focus on short-term goals and how to adress the issues. | [2]: Hidden Stratification Causes Clinically Meaningful Failures in Machine Learning for Medical Imaging: Introduces a concept reminding me of Simpson’s paradox, where a model outperforms humans (or another model) in the average, but underperforms for a given subset, which may be critical for the outcome of using the model. | [3]: Evaluating classification models: This four part Medium series highlights classification model metrics, arguing for a single evaluation metric, but gives examples of different use cases for preferring different metrics. | . The problem . Metrics in machine learning . Most AI algorithms are based on optimizing metrics, and due to algorithms optimizing blindly, and too efficiently, a model that scores better on metrics may often lead to outcomes that are far from optimal. The paper cites Goodhart’s law: . When a measure becomes a target, it ceases to be a good measure. . Goodhart’s law arose in the 1970s after attemps to slow down inflation, by choosing metrics with stable relationships to inflation as targets for central banks. However, the relationships between the chosen metrics and inflation broke down when the metrics were chosen as targets. The law arose from observing human behaviour, but an algorthim will optimize more efficiently as is therefore more prone to following this law. . As training a model is explicitly defined around optimizing a specific metric, such as accuracy or error rate, we knowingly or unknowingly make priotizations in our problem, such as . whether or not we can give partial credit for some answers | whether or not false positives are equally weighted as false negatives | whether or not we penalize frequent medium errors the same as rare large errors | . Four problems with metrics . The problems found by case study review in [1] are: . Metrics are proxies They may be invalid in corner cases, extreme cases or represent a non-causal relationship, for example: When we want to assess crime, we measure arrests. | We don’t always realize that we are measuring a proxy, i.e. that our dataset does not contain features that are actually correlated with the goal. An example given was a study to investigate stroke patients, that ended up finding patients who were able to use the health care services. | . | Metrics will be gamed, not only in model selection but by models themselves. In reinforcement learning, two types of gaming are common: gaming metrics: exploiting a poor definition of metrics | finding glithces in implementations of environment or reward function | . | Recommender algorithms are also prone to be gamed as adversarial attacks | . | Metrics over-emphasize short-term goals A common example is the click-through rate which does not tell us anything about long term effects on readers’ behaviour | YouTube &amp; Facebook both have examples of promoting horrible content, which eventually damages hiring abilities for the companies | . | Many online metrics are gathered in highly addictive environments | . Hidden stratification . Another problem described in [2] as hidden stratification manifests as a model outperforming humans in the aggregate, but underperforming in a critical segment, i.e. there are hidden subsets of the data where performance is poor. Many metrics will not reveal this, as they are dominated by larger subsets. . Hidden stratification is particularly important in medical research, and other fields where the cost of false negatives can be far higher than the cost of false positives (or vice versa). An example is a model that on average outperforms human in classifying scans as cancerous or healthy, but underperforms for the most aggressive cancer types. If the task was allocated to the model alone, this could in fact lead to higher mortality. . Whether or not hidden stratification poses a problem is identified through the dataset, likely progression of events and consequences: . imbalanced classes, where the most serious events happen rarely | rapidly developing complications | imbalance in consequences, where consequences of one class is far more serious than other classes. | . Different structures of subclasses contribute to degraded performance: . Low subclass prevalence | Reduced accuracy of labels in the subclass | Subtle discriminative features | Spurious correlations | . Proposed solutions and mitigating measures . Three solutions to adress weaknesses of metrics . Solutions proposed in [1] are: . Use several different metrics, to prevent gaming and to obtain a more robust basis for evaluation, for example by using Metrics measuring different proxies of the same goal | Metrics measuring the same proxy for different time horizons | . | Combine metrics with qualitative accounts. Two concrete suggestions for accounts accompanying metrics are Model cards for model reporting are fact sheets containing additional information on trained models, aimed to reduce unintended side effects of using models wrongly. The cards are intended for model developers, software developers, impacted individuals, policy makers etc. containing descriptions of (among other details): Intended use, intended users and out-of-scope use cases | Metrics, decision thresholds and how uncertainty in metrics has been assessed | Details on the evaluation dataset and training dataset if possible: description, preprocessing, motivation behind chosen datasets | Factors: groups of observations in dataset, instrumentation of observations, environment where data has been collected | . | Datasheets for datasets are fact sheets to facilitate communication between dataset creators and dataset consumers answering questions within the topics Motivation (what problem prompted the creation, who created it, with support from whom) | Composition (Is it a sample or a complete set, labels, missing information, recommended split, error sources, is it self contained) | Collection process (directly observed/self reported/inferred, time frame for collection, sampling strategy if any) | Preprocessing (discretization, binning, removal of observations, processing of missing values, link to raw data, link to software for data cleaning) | Uses (Tasks the dataset is currently or previously used for, potential other tasks, potential impact to future uses from creation process of the dataset) | Distribution (Availability and sensitivity of dataset) | Maintenance (Who is responsible, will the dataset be updated, erratum, availability of older versions, procedures if users want to extend the dataset) | . | . | Involve different stakeholders in the initial metric development. Model cards suggests involving stakeholders by teaching the implications of an already existing model, but [1] suggests even involving stakeholders in development of metrics. | . Measuring hidden stratifications so it can be addressed . [2] proposes three strategies for measuring hidden stratifications, which then need to be addressed by models in different ways. . Schema completion: A schema author defines all subsets that need to be labeled, and performs labeling on the test dataset. Enables accurate reporting | Helps guide model development | Time consuming | Limited by the knowledge of the schema author | . Experiments showed substantial differences in AUC score for different subsets identified by a medical professional, for a dataset exhibiting low subclass prevalence and subtle discriminative features as well as one exhibiting poor label quality and subtle discriminative features. . | Error auditing: An auditor examines model output in search of irregularities such as consistently incorrect predictions on a recognizable subclass. Not limited by the expectations of a schema author | Only concerning subclasses need to be examined, thus more labor-efficient than schema completion | Limited by the weaknesses identified by the auditor, therefore not as exhaustive a search as the schema completion | Limited by errors prevalent in the test set, may not be representative for small subsets | . The method was tested on a dataset with spurious correlations. A particular subset was found to be prevalent in the test set false negatives, and labeled each observation in the test set accordingly. The spurious correlation factor in the poorly performing subset, was found to have a corresponding superset without the spurious correlation factor with high AUC scores. . | Algorithmic measurement: An algorithmic search for for subclasses, such as clustering. Less dependent on a human identifying all relevant subsets | Can reduce burden on a human analyst | Efficiency is limited by the difficulty of separating subclasses in the feature space of the analysis | Still requires human review | . Concretely, a simple k-means algorithm was applied for k from 2 - 5. For each k, the two clusters (with more than 100 observations) with the largest difference in error rate was identified. From these four pairs, the one with the largest Euclidian distance between centres is chosen. The method is not always successful in producing well-separated clusters for clinically meaningful subclasses, but may be useful with improved clustering algorithms, or in addition to one of the other methods. . | . Combining metrics to a single evaluation metric . [3] seemingly opposes [1] and [2] and recommends that we decide on one single metric in order to be able to iterate quickly and develop and rank new models, and argues that using a slate of different metrics is more suited for model diagnostics than evaluation. This approach is also recommended by the famous Andrew Ng for model development. . [3] discusses metrics for classification models, where we measure false positives and false negatives, bringing us to precision and recall, precision being the fraction of positive predictions that are correct, and recall being the fraction of predictions on the positive class that are correct. Two different models are examined, one with better precision, and one with better recall. When choosing between the two, we choose how much better precision or recall we want to get, by sacrificing the other. . To get to a single metric, the article proposes different combinations of precision P and recall R: . Linear relationships Weighted arithmetic mean: A simple arithmetic mean is just 0.5P + 0.5R. We would weight precision and recall equally, always being willing to trade one unit of precision for one unit of recall. We could weight precision and recall differently, for example by saying we will trade one unit of precision for a units of recall, indicating for a &gt; 0 that we care more about precision than recall. This represents a linear relationship between the trade-off. If a = 2, a model with a precision of 80 % and recall of 40 % is equally good as a model with precision of 70 % and recall of 60 %. | . | Non-linear relationships . The article proposes several different non-linear combinations: . Weighted harmonic mean | Weighted geometric mean | Weighted power mean | . With a non-linear relationship, the relative importance of precision and recall depend on their current values. When recall is low, recall is valued higher than precision and vice versa. A geometric mean is closer to the linear relationship than the harmonic mean. Whereas the geometric and harmonic mean, had constant preference lines that curved upwards for all parameter values, the power means are more flexible, and curvature of the constant preference lines shift from curving upwards to downwards as we vary parameters. . | . The author argues that whether or not arithmetic means are sufficient depends on the nature of the problem. In clinical situations, arithmetic means are often sufficient. Each individual experiences one single prediction from the model. The author works in retail, where the rate of false positive to false negative classifications is valued differently based on the level of false positives, and each individual experiences a range of predictions from the model. As the author puts it, it is not each single prediction that matters, but the overall impression, as is typical for information retrieval problems. For example, at a high recall (we are classifying most dresses as dresses), another point of recall isn’t that important, because a customer will find the dress they are looking for, and it is more important to improve precision, such that there are fewer false positives. Equivalenty, if precision is very high, it is probably more important to work on the recall, making sure the customer can be exposed to all dresses, rather than making sure that the customer is not exposed to something which is not a dress. . Conclusion . It seems that everyone has a favorite urban legend of how a metric was gamed or lead to unforseen consequences, be it from human experience or a reinforcement learning agent. The three articles seem to assess the situation differently: . [1] proposes different ways to enrich simple metrics: using several metrics, using qualitative accounts in addition, and letting stakeholders take part in defining metrics. Spending more time defining and reviewing effects of metrics with stakeholders it probably time well spent, although iteration cycles for models will be longer with a slate of metrics. Although I like the idea of explicitly stating the intended purpose and out-of-scope use cases for a model, I wonder how it should be implemented in order to keep it updated and helpful for relevant end users. | [2] does not directly propose a solution, but exposes a problem and the settings in which we should investigate whether our models have hidden stratifications that may worsen end outcomes although performance metrics show improvements. Although these problems do not always lead to fatal outcomes, investigating models to find subdomains of poor performance is a good practice to improve models, and should be practiced systematically. | Whereas [1] and [2] show weaknesses of single metric evaluation, [3] argues for the benefits to model development iteration cycles. I find this very compelling. Although I haven’t used the particular metrics described in the series in practice, the concept of experimenting with finding and reviewing which metric is used seems to mediate some of the shortcomings of single metrics. Using several metrics lead us to implicitly weighing them against eachother, whereas combining metrics to a single number leads to an explicit definition, which again leads to concrete and transparent examples which we can discuss with stakeholders. | . It seems to me that a single metric for evaluation can be a recipe for disaster or success, depending on the execution, transparency and willingness to adjust. If done correctly, I think it leverages some of the benefits found in the metrics article, while still obtaining a simple way to evaluate experiments and quickly develop and test new hypotheses. If done poorly, it is just another metric that can be gamed. .",
            "url": "https://gunnhildsp.github.io/notes/metrics/data-science/2020/03/07/metrics.html",
            "relUrl": "/metrics/data-science/2020/03/07/metrics.html",
            "date": " • Mar 7, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "These are my notes, taken as I try to learn the things I google. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://gunnhildsp.github.io/notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}