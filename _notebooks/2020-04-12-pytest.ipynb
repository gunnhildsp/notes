{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with pytest\n",
    "> `pytest` is a widely used Python framework for testing, which is flexible for small and large test suites.\n",
    "- toc: true\n",
    "- categories: [python, pytest, testing] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You should test your code \n",
    "There are many reasons why you should test your code:\n",
    "* Writing a test helps you define what your code should do and helps you enforce single responsibility of functions. \n",
    "* Getting into a habit of writing tests for corner cases helps you develop more robust code.\n",
    "* If your tests have good names and reasonable test cases it serves as documentation of your code for your future self or other collaborators. \n",
    "* It is a good way of documenting assumptions you make. It is often useful to write a test that fails in the event of an assumption being broken, such as functionality you have not implemented yet. \n",
    "* Many IDEs have good built-in support for running tests, and debugging a test is often my main way of either debugging through my own code to find errors, or stepping through code that is unfamiliar to me to se how the code is supposed to work and fail. \n",
    "* Good test coverage is essential when refactoring code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put your tests where `pytest` can find them  \n",
    "`pytest` finds your tests automatically according to (what I have just learned is) [standard test discovery](https://docs.pytest.org/en/latest/goodpractices.html#conventions-for-python-test-discovery) by: \n",
    "* recursively looking through directories\n",
    "* search for files named `test_*.py` or `*_test.py`\n",
    "* in those files: search for functions prefixed with `test` outside classes\n",
    "* in those files: search for functions or methods inside classes prefixed with `Test`\n",
    "\n",
    "An example of a directory structure could look like this: \n",
    "```\n",
    "my_code/\n",
    "    app.py\n",
    "    utils.py\n",
    "tests/\n",
    "    test_app.py\n",
    "    test_utils.py\n",
    "```\n",
    "\n",
    "It is good practice to organize your tests separately from the rest of your code, for example in a folder named `tests` as above. There are [many reasons](https://blog.ionelmc.ro/2014/05/25/python-packaging/#the-structure), for example default module discovery may ignore your tests, your tests may require additional packages to run, and if you are writing a library or application, the tests should not need to be included in your library or application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your tests\n",
    "I normally use PyCharm and find the built-in functionality for running and debugging tests from the interface quite nice, but it is always cool to learn more CLI tricks. \n",
    "\n",
    "### Run all tests\n",
    "Running all tests found from the current directory is quite simple: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini\n",
      "collected 6 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_examples.py \u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                      [ 33%]\u001b[0m\n",
      "test_fixturefunctions.py \u001b[32m.\u001b[0m\u001b[31m                                               [ 50%]\u001b[0m\n",
      "test_mark_examples.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_____________________________ test_failing_example _____________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_failing_example\u001b[39;49;00m():\n",
      "        \u001b[96mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mHello\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_examples.py\u001b[0m:9: AssertionError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "Hello\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_examples.py::test_failing_example - assert False\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m5 passed\u001b[0m\u001b[31m in 0.83s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We can run the tests by running either `python -m pytest` or just `pytest`. Running through python will add the current directory to `sys.path` which is often desirable, therefore I'll stick with that.  \n",
    "\n",
    "Here, `pytest` discovered three file with tests, `test_examples.py`, containing two tests, one which passes and one which fails, `test_fixturefunctions.py`, containing one passing test and `test_mark_examples.py`, containing three passing tests. The tests in `test_examples.py` look like this: \n",
    "```python\n",
    "# contents of test_examples.py\n",
    "def test_example():\n",
    "    print(\"Hi\")\n",
    "    assert True\n",
    "    \n",
    "def test_failing_example():\n",
    "    print(\"Hello\")\n",
    "    assert False\n",
    "\n",
    "```\n",
    "### Customize test output\n",
    "The default mode is that output from a test is not shown unless the test fails. We can use the `capture` option to **print output** anyway, or `-s` for short:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_examples.py Hi\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_examples.py::test_example --capture=no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traceback formatting for failing tests is set by the option `tb`. There are many [options](https://docs.pytest.org/en/latest/usage.html#modifying-python-traceback-printing), such as `--tb=line` to limit output from failing tests to one line: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "test_examples.py \u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                      [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "/Users/Gunnhild/code/notes/_notebooks/test_examples.py:9: assert False\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_examples.py::test_failing_example - assert False\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.06s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_examples.py --tb=line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify modules, files or single tests\n",
    "Above we used a trick: We can **run tests found in a single file and a single test** from the command line as well, using the syntax \n",
    "```\n",
    "pytest test_module/test_file_name.py::test_function_name\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/Gunnhild/code/notes/_notebooks\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_examples.py \u001b[32m.\u001b[0m\u001b[32m                                                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_examples.py::test_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group tests using marks\n",
    "We can use [marks](https://docs.pytest.org/en/latest/mark.html) to run groups of tests easily with the `-m` option\n",
    "```\n",
    "python -m pytest -m mark_name\n",
    "```\n",
    "`pytest` has a range of built-in marks, such as the `slow` mark. This can be used to group tests so that you can run the quick tests and check for failures there first, before running the slow tests. We register our marks in `pytest.ini` to let pytest know we are marking on purpose, otherwise pytest will raise a Warning. \n",
    "For example, we could mark tests for different purposes: \n",
    "```python\n",
    "# contents of test_examples.py\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.this\n",
    "def test_example():\n",
    "    print(\"Hello\")\n",
    "    assert True\n",
    "\n",
    "@pytest.mark.this\n",
    "@pytest.mark.that\n",
    "def test_several_marks():\n",
    "    print(\"Nothing\")\n",
    "    assert True\n",
    "\n",
    "def test_unmarked():\n",
    "    print(\"Hello\")\n",
    "    assert 1\n",
    "```\n",
    "Our pytest.ini should then look like \n",
    "```\n",
    "# content of pytest.ini\n",
    "[pytest]\n",
    "markers =\n",
    "    this: example of marker.\n",
    "    that: another example of marker.\n",
    "```\n",
    "and then we can run groups accordingly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini\n",
      "collected 6 items / 4 deselected / 2 selected                                  \u001b[0m\u001b[1m\n",
      "\n",
      "test_mark_examples.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m4 deselected\u001b[0m\u001b[32m in 0.75s\u001b[0m\u001b[32m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -m this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini\n",
      "collected 6 items / 5 deselected / 1 selected                                  \u001b[0m\u001b[1m\n",
      "\n",
      "test_mark_examples.py \u001b[32m.\u001b[0m\u001b[32m                                                  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m5 deselected\u001b[0m\u001b[32m in 0.59s\u001b[0m\u001b[32m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -m \"this and not that\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use fixtures to initialize your test\n",
    "In software testing, a [fixture](https://docs.pytest.org/en/latest/fixture.html) can be used to ensuring that tests are repeatable: the same code with the same inputs in the same environment will reproduce the same results. We can use fixtures for \n",
    "* setting up mocks of external services such as APIs, so your tests won't depend on the reliability of external applications, and your can test all response cases you need to\n",
    "* setting up and sharing test data between tests\n",
    "* setting up the environment that the test will run in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have a function that saves an input dataframe to a specified path as a csv file. \n",
    "```python\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "def save(df: pd.DataFrame, save_path: Path):\n",
    "    if not df.empty:\n",
    "        df.to_csv(save_path)\n",
    "    else:\n",
    "        print(\"Nothing to save. \")\n",
    "```\n",
    "To test this function, we might want to save a dataframe, and check that we get the same result back when we read the csv file. For this test we use the built-in fixture `tmp_path`. The `tmp_path` fixture creates a path unique to each test run, that doesn't clutter the repository or any other shared folders we might care about. This ensures that if the tests are run in a different environment, such as on another developer's computer or in a continuous integration pipeline, the folders will exist when needed and be deleted eventually. We use any fixture in a test by using the fixture name as an input argument to the test function: \n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "\n",
    "def test_save(tmp_path):\n",
    "    # Given\n",
    "    save_path = tmp_path / \"df.csv\"\n",
    "    df_expected = pd.DataFrame(columns=[\"a\", \"b\", \"c\"], data=[3, 2, 1]*np.ones([5,3]))\n",
    "    save(df_expected, save_path)\n",
    "    \n",
    "    # When \n",
    "    df_actual = pd.read_csv(save_path, index_col=False)\n",
    "\n",
    "    # Then \n",
    "    assert_frame_equal(df_expected, df_actual)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "test_save_example.py \u001b[32m.\u001b[0m\u001b[32m                                                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.96s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_save_example.py::test_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test data in fixtures \n",
    "\n",
    "Right now we create the test data in the test. An alternative is to create the dataframe in a fixture. The advantage is that there is less code to read in the test, and the fixture can be reused by different tests, if we have several functions acting on the data we can avoid duplication of code. We create fixtures by using the decorator `@pytest.fixture`:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.testing import assert_frame_equal\n",
    "from pathlib import Path\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.fixture()\n",
    "def test_dataframe():\n",
    "    df = pd.DataFrame(columns=[\"a\", \"b\", \"c\"], data=[3, 2, 1]*np.ones([5,3]))\n",
    "    return df\n",
    "    \n",
    "    \n",
    "def test_save_fixturized(tmp_path, test_dataframe):\n",
    "    # Given\n",
    "    save_path = tmp_path / \"df.csv\"\n",
    "    save(test_dataframe, save_path)\n",
    "\n",
    "    # When \n",
    "    df_actual = pd.read_csv(save_path, index_col=False)\n",
    "\n",
    "    # Then \n",
    "    assert_frame_equal(test_dataframe, df_actual)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "test_save_example.py \u001b[32m.\u001b[0m\u001b[32m                                                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.84s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_save_example.py::test_save_fixturized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we call the fixture function, we automatically get the return value, instead of the function itself, i.e. we do not need to use assign the return value of the function to a variable holding the dataframe: `df_expected = test_dataframe()` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `monkeypatch` to set environment variables \n",
    "We often use environment variables to configure our functionality, such as where they should output their results, login credentials for databases and services. Keeping these configs in environment variables is [recommended](https://12factor.net/config) in order to run the same code with different configurations in different environments: locally when developing, in a test environment and in a production environment. To test these functions, we can use [monkeypatching](https://docs.pytest.org/en/latest/monkeypatch.html). Let's say we read environment variables in our function: \n",
    "```python\n",
    "import os\n",
    "\n",
    "\n",
    "def read_config():\n",
    "    password = os.environ[\"DB_PASSWORD\"]\n",
    "    user = os.environ[\"DB_USER\"]\n",
    "    return {\n",
    "        \"password\": password, \n",
    "        \"user\": user,\n",
    "    }\n",
    "```\n",
    "\n",
    "We can then use the monkeypatch fixture in our test, to set environment variables to toy values for the test execution: \n",
    "```python\n",
    "def test_read_config(monkeypatch):\n",
    "    monkeypatch.setenv(\"DB_PASSWORD\", \"password123\")\n",
    "    monkeypatch.setenv(\"DB_USER\", \"username\")\n",
    "    conf = read_config()\n",
    "    assert set(conf.keys()) == {\"password\", \"user\"}\n",
    "    assert conf[\"password\"] == \"password123\"\n",
    "    assert conf[\"user\"] == \"username\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_monkeypatching.py \u001b[32m.\u001b[0m\u001b[32m                                                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_monkeypatching.py::test_read_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could extract the mocking into fixtures to share the setup between tests: \n",
    "```python\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.fixture()\n",
    "def monkeypatch_config(monkeypatch):\n",
    "    monkeypatch.setenv(\"DB_PASSWORD\", \"password123\")\n",
    "    monkeypatch.setenv(\"DB_USER\", \"username\")\n",
    "\n",
    "    \n",
    "def test_read_config_using_fixture(monkeypatch_config):\n",
    "    conf = read_config()\n",
    "    assert set(conf.keys()) == {\"password\", \"user\"}\n",
    "    assert conf[\"password\"] == \"password123\"\n",
    "    assert conf[\"user\"] == \"username\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_monkeypatching.py \u001b[32m.\u001b[0m\u001b[32m                                                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_monkeypatching.py::test_read_config_using_fixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use mocks to test external dependencies \n",
    "When we have external dependencies, such as an API or databases, we want our tests to be independent of the status of our dependencies. For instance, we want to test that our code can handle both when the API is up and running normally, and when the API is down. However, we can't control whether the API is up or down when we run our tests, so we use *mocks* to imitate the responses from our dependencies. \n",
    "\n",
    "Mocking is a field big enough for it's own post at some point, but what I keep coming back to is a RealPython article on [Understanding the Python Mock Object Library](https://realpython.com/python-mock-library/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use parametrization to cover multiple cases\n",
    "There are at least two ways of rerunning tests for different test cases in order to ensure all execution paths are tested, and both involve parametrizing: \n",
    "* [Parametrizing fixtures](https://docs.pytest.org/en/latest/fixture.html#fixture-parametrize)\n",
    "* [Parametrizing tests](https://docs.pytest.org/en/latest/parametrize.html#pytest-mark-parametrize-parametrizing-test-functions)\n",
    "\n",
    "When we parametrize, pytest will run the tests for all different cases we specify automatically.\n",
    "\n",
    "In my experience, we should parametrize tests to ensure that we cover all the different cases that arise from having different input data to the function under test, i.e. the function specific stuff, whereas we should parametrize fixtures when we want to test different objects. If the fixtures are mocking external dependencies or our own complex objects, it may be a good idea to parameterize fixtures to ensure we cover different setups. \n",
    "\n",
    "A code smell indicating that we should parametrize a fixture, is duplicated code for creating different tests for different functions, or setting up different test cases in the same test, across multiple tests. A nice side effect of parametrizing your fixtures, is that all new tests that use the same fixture will automatically be run for the different cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametrizing fixtures to cover multiple test cases \n",
    "Let's go back to the `save` test example of saving a dataframe. \n",
    "\n",
    "> Note: In this test, I have parametrized an input parameter to the function, but above I argued that input arguments is better suited for test parametrization than fixture parametrization. A better example would perhaps be if the data in the test was an attribute of a class, and we wished to create a mock of the class to test. It may also be suitable to extract input parameters to fixtures when creation is complex. In any case, the example serves to show some of the functionality of fixtures that we can use. \n",
    "\n",
    "Where we left off, our test only covered one execution path: the first branch of the if statement, i.e. if the input dataframe is non-empty. If we want to test the other branch, we can parametrize the fixture to return different dataframes. When a test relies on a parametrized fixture, it will be rerun for all parametrizations of the fixture. \n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.testing import assert_frame_equal\n",
    "import pytest \n",
    "\n",
    "@pytest.fixture(params=[True, False], ids=[\"non-empty\", \"empty\"])\n",
    "def dataframes(request):\n",
    "    if request.param:\n",
    "        return pd.DataFrame(columns=[\"a\", \"b\", \"c\"], data=[3, 2, 1]*np.ones([5,3]))\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "\n",
    "def test_save_parametrized_fixture(tmp_path, dataframes):\n",
    "    # Given\n",
    "    save_path = tmp_path / \"df.csv\"\n",
    "    save(dataframes, save_path)\n",
    "\n",
    "    if dataframes.empty:\n",
    "        # When \n",
    "        files_in_dir = [x for x in tmp_path.iterdir() if x.is_file()]\n",
    "        # Then\n",
    "        assert not files_in_dir\n",
    "        \n",
    "    else:\n",
    "        # When\n",
    "        df_actual = pd.read_csv(save_path, index_col=False)\n",
    "        # Then \n",
    "        assert_frame_equal(dataframes, df_actual)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini\n",
      "collected 2 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_save_example.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.60s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_save_example.py::test_save_parametrized_fixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This executes the test twice automatically. We use the `params` keyword to parametrize our fixture, and the `ids` keyword to provide human readable names for our different parametrizations. We use the `request` fixture in our fixture to access the parameters we send in on the `request`'s attribute `param`.\n",
    "\n",
    "`params` takes a list as inputs, so if we need several arguments to our fixture function, we can use for example a list of tuples or a list of dicts:  \n",
    "```python\n",
    "@pytest.fixture(params=[(True, 5), (False, )], ids=[\"non-empty\", \"empty\"])\n",
    "def df_fixture_with_tuples(request):\n",
    "    if request.param[0]:\n",
    "        n = request.param[1]\n",
    "        return pd.DataFrame(columns=[\"a\", \"b\", \"c\"], data=[3, 2, 1]*np.ones([n,3]))\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "\n",
    "@pytest.fixture(\n",
    "    params=[\n",
    "        {\"non_empty\": True, \"length\": 5}, \n",
    "        {\"non_empty\": False, \"length\": None}\n",
    "    ], \n",
    "    ids=[\"non-empty\", \"empty\"]\n",
    ")\n",
    "def df_fixture_with_dict(request):\n",
    "    if request.param[\"non_empty\"]:\n",
    "        n = request.param[\"length\"]\n",
    "        return pd.DataFrame(columns=[\"a\", \"b\", \"c\"], data=[3, 2, 1]*np.ones([n,3]))\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run a failing test, to see our `id` in action, with this toy test function\n",
    "```python\n",
    "def test_demo_fail_output(dataframes):\n",
    "    if dataframes.empty:\n",
    "        assert False\n",
    "    else: \n",
    "        assert True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini\n",
      "collected 2 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_save_example.py \u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                  [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "/Users/Gunnhild/code/notes/_notebooks/test_save_example.py:74: assert False\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_save_example.py::test_demo_fail_output[empty] - assert False\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.76s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest --tb=line test_save_example.py::test_demo_fail_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `id` of the failing test, `empty`, is printed in the list of failed tests. If you use PyCharm, you will find that  it prints a pretty summary of the ids of parametrized tests, both parametrized through fixtures and the test itself, by building up a tree of the tests that are run, organised by module, script, and function, and I'm sure many other IDEs have similar functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametrize tests to cover multiple test cases\n",
    "To cover the different execution paths, we can also parametrize the test itself, which looks a little different. Let's return to our save example, but add to the functionality. Let's say we want to pass an argument for the number of rows to save, and add a validator to check that the number of rows is a valid argument: \n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def save(df: pd.DataFrame, save_path: Path, num_rows: Optional[int] = None):\n",
    "    if not df.empty: \n",
    "        if num_rows: \n",
    "            num_rows = validate_num_rows(num_rows)\n",
    "            df = df[0: num_rows]\n",
    "        df.to_csv(save_path, index=False)\n",
    "    else:\n",
    "        print(\"Nothing to save. \")\n",
    "\n",
    "def validate_num_rows(num_rows)-> int:\n",
    "    if not int(num_rows) == num_rows:\n",
    "        raise ValueError(f\"num_rows must be int, got {num_rows}\")\n",
    "    if num_rows < 1:\n",
    "        raise ValueError(f\"num_rows must be >= 1, got {num_rows}\")\n",
    "    return int(num_rows)\n",
    "```\n",
    "\n",
    "We parametrize our test to cover both the case when a `num_rows` argument is not supplied, and when it is supplied:\n",
    "```python\n",
    "import pandas as pd\n",
    "import pytest \n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(argnames=\"number_of_rows\", argvalues=[None, 3])\n",
    "def test_save_fixturized(tmp_path, test_dataframe, number_of_rows):\n",
    "    save_path = tmp_path / \"df.csv\"\n",
    "    save(test_dataframe, save_path, num_rows=number_of_rows)\n",
    "\n",
    "    df_actual = pd.read_csv(save_path)\n",
    "    if number_of_rows:\n",
    "        df_expected = test_dataframe[0:number_of_rows]\n",
    "    else: \n",
    "        df_expected = test_dataframe\n",
    "        \n",
    "    assert_frame_equal(df_expected, df_actual)\n",
    "```\n",
    "Parametrize is a mark, where the first argument, `argnames`, is a string with the argument names separated by commas, the second, `argvalues` is a list with the argument values for the different test cases. If we have several arguments, `argvalues` must be a list of tuples, and the number of tuples must match the number of `argnames` for each element of the list. We use the parameterized values in the test by setting them as input arguments to the test. These names must match `argnames`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini\n",
      "collected 2 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_parametrize.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.79s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_parametrize.py::test_save_fixturized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing that exceptions are raised\n",
    "To make assertions about expected exceptions, we use [`pytest.raises`](https://docs.pytest.org/en/latest/assert.html#assertraises). We will use the function `validate_num_rows` as an example, as it raises errors in some cases, and not in others. This is also a good opportunity to document some assumptions for our future self about what this test does. Since there are many different cases, we will parametrize the test function to cover all branches of the code and demonstrate functionality. \n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"input_num_rows, expected_output, expected_error\", \n",
    "    [\n",
    "        (3, 3, None),\n",
    "        (2.0, 2, None),\n",
    "        (\"text\", None, ValueError),\n",
    "        (\"3.4\", None, ValueError),\n",
    "        (-1, None, ValueError),\n",
    "    ], \n",
    "    ids=[\n",
    "        \"integer\",\n",
    "        \"float_that_can_be_converted_to_integer\",\n",
    "        \"string_fails\",\n",
    "        \"float_fails\",\n",
    "        \"negative_number_fails\"\n",
    "    ]\n",
    ")\n",
    "def test_validate_num_rows(input_num_rows, expected_output, expected_error):\n",
    "    if expected_error:\n",
    "        with pytest.raises(expected_error):\n",
    "            validate_num_rows(input_num_rows)\n",
    "    else:\n",
    "        actual_output = validate_num_rows(input_num_rows)\n",
    "        assert expected_output == actual_output\n",
    "```\n",
    "\n",
    "Now we have an example of having multiple argument names and argument values with tuples, as mentioned above. \n",
    "\n",
    "To test for failure and success, we use the argument `expected_error` which we set to `None` for the test cases that should fail and to the error we expect when a test should pass. Then we use `pytest.raises` to call a function and validate that the expected error was thrown if `expected_error` is not `None`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/Gunnhild/code/notes/_notebooks, inifile: pytest.ini\n",
      "collected 5 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_parametrize.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 0.56s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_parametrize.py::test_validate_num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option for conditional raising of exeptions is shown [in the documentation](https://docs.pytest.org/en/latest/example/parametrize.html#parametrizing-conditional-raising), and uses a contextmanager that yields for non-failing cases. It seems a little complicated to me, but if you're used to this construction you can save some lines of code in your tests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading \n",
    "*Mocking* is an obvious next step when writing tests, my favorite source is the above mentioned RealPython article on [Understanding the Python Mock Object Library](https://realpython.com/python-mock-library/).\n",
    "\n",
    "*Code coverage* is a concept that goes hand in hand with testing and is a good starting point for what to test. [`pytest-cov`](https://pytest-cov.readthedocs.io/en/latest/) is an easy coverage plugin for pytest. \n",
    "\n",
    "The ecosystem of plugins to pytest is huge, and there are many I would like to try, for example[`pytest-mock`](https://github.com/pytest-dev/pytest-mock/) for mocking and [`pytest-vcr`](https://pytest-vcr.readthedocs.io/en/latest/) for HTTP requests. [This tutorial](https://joshpeak.net/posts/2019-06-18-Advanced-python-testing.html) covers both the `pytest-vcr` library, but also basic concepts in testing and code quality, as well as the author's strategy on how to read up on testing in Python. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
